{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ad4d01-0946-4ad3-acc1-aba2e2b28ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accommodations: 28\nTotal Incidents: 30\nOverall Accommodation-to-Incident Ratio: 0.9333333333333333\nElapsed time: 266.804359 seconds\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "def validate_and_map(row):\n",
    "    # row is [line_number, c0, c1]\n",
    "    if len(row) >= 3:\n",
    "        return (row[1], (row[0], row[2]))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def inside_accomodation(x_position, left, right):\n",
    "    return float(x_position) > left and float(x_position) < right\n",
    "\n",
    "def inside_incident(x_position, left, right):\n",
    "    return float(x_position) > left and float(x_position) < right\n",
    "\n",
    "def find_accomodation_or_incident(id, values, cutoff_index, accom_left, accom_right, inc_left, inc_right):\n",
    "    if not values:\n",
    "        return 0, 0\n",
    "\n",
    "    sorted_values = sorted(values, key=lambda x: int(x[0]))\n",
    "    accomodation_count = 0\n",
    "    incident_count = 0\n",
    "    current_state = -1\n",
    "\n",
    "    for line, x_position in sorted_values:\n",
    "        if inside_accomodation(x_position, accom_left, accom_right):\n",
    "            if current_state == 1:\n",
    "                accomodation_count += 1\n",
    "                current_state = 2\n",
    "        elif inside_incident(x_position, inc_left, inc_right):\n",
    "            if current_state == 0 and int(line) <= cutoff_index:\n",
    "                incident_count += 1\n",
    "                current_state = 1\n",
    "        else:\n",
    "            current_state = 0\n",
    "\n",
    "    return incident_count, accomodation_count\n",
    "\n",
    "# Start\n",
    "start_time = time.time()\n",
    "printable_string = ''\n",
    "\n",
    "set_count_to_deduct_for_incident = 500\n",
    "\n",
    "# Read from Databricks table\n",
    "df = spark.table(\"hive_metastore.default.mydata_2_txt\")\n",
    "# Assume columns: c0 (id as string), c1 (position as string)\n",
    "rdd = df.rdd.map(lambda row: (row['_c0'], row['_c1']))\n",
    "\n",
    "# Add line numbers using zipWithIndex\n",
    "indexed_rdd = rdd.zipWithIndex().map(lambda x: [x[1] + 1, x[0][0], x[0][1]])\n",
    "\n",
    "# Filter only lines that contain 'NaN'\n",
    "nan_lines_rdd = indexed_rdd.filter(lambda x: 'NaN' in x)\n",
    "\n",
    "# Get the first NaN line if exists, else use dataset length\n",
    "first_nan = nan_lines_rdd.take(1)  # This does not collect entire RDD, just the first NaN line\n",
    "if first_nan:\n",
    "    cutoff_index = first_nan[0][0]  # line number of the first NaN occurrence\n",
    "else:\n",
    "    # If no NaN found, the cutoff is the entire dataset length\n",
    "    total_lines = indexed_rdd.count()  # Count of the entire dataset\n",
    "    cutoff_index = total_lines\n",
    "\n",
    "# If we need to adjust cutoff_index based on the last 500 NaN lines:\n",
    "nan_line_numbers_rdd = nan_lines_rdd.map(lambda x: x[0])\n",
    "nan_count = nan_line_numbers_rdd.count()\n",
    "\n",
    "if set_count_to_deduct_for_incident > 0 and nan_count >= set_count_to_deduct_for_incident:\n",
    "    # Get the highest (largest line number) NaN lines using 'top'\n",
    "    last_500_nan = nan_line_numbers_rdd.top(set_count_to_deduct_for_incident)\n",
    "    # This returns up to 500 largest NaN line numbers.\n",
    "    # We want the smallest line number among these \"top\" NaN lines as the cutoff.\n",
    "    new_cutoff = min(last_500_nan)\n",
    "    cutoff_index = new_cutoff\n",
    "\n",
    "# Filter out lines with NaN for further processing\n",
    "accomodation_cleaned_rdd = indexed_rdd.filter(lambda row: 'NaN' not in row)\n",
    "accomodation_key_value_rdd = accomodation_cleaned_rdd.map(validate_and_map).filter(lambda x: x is not None)\n",
    "accomodation_grouped_rdd = accomodation_key_value_rdd.groupByKey()\n",
    "\n",
    "# Define planes\n",
    "accomodation_plane_Right = 73.2186\n",
    "accomodation_plane_Left = 2.709\n",
    "incident_plane_Right = accomodation_plane_Right + 20\n",
    "incident_plane_Left = accomodation_plane_Left - 20\n",
    "\n",
    "accomodation_rdd_result = accomodation_grouped_rdd.map(\n",
    "    lambda x: [x[0], find_accomodation_or_incident(\n",
    "        x[0], list(x[1]), cutoff_index,\n",
    "        accomodation_plane_Left, accomodation_plane_Right,\n",
    "        incident_plane_Left, incident_plane_Right\n",
    "    )]\n",
    ")\n",
    "\n",
    "total_counts = accomodation_rdd_result.map(lambda x: x[1]).reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "total_incidents, total_accommodations = total_counts\n",
    "overall_ratio = total_accommodations / total_incidents if total_incidents != 0 else \"Infinity\"\n",
    "\n",
    "# Print results\n",
    "printable_string += f\"Total Accommodations: {total_accommodations}\\n\"\n",
    "printable_string += f\"Total Incidents: {total_incidents}\\n\"\n",
    "printable_string += f\"Overall Accommodation-to-Incident Ratio: {overall_ratio}\\n\"\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "printable_string += f\"Elapsed time: {elapsed_time:.6f} seconds\\n\"\n",
    "\n",
    "print(printable_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a25991a6-82d4-475b-a67a-fad4348f650e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DatabricksNotebook_Parallel Programming Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
